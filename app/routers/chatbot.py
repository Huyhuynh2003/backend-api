from fastapi import APIRouter
from pydantic import BaseModel
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

# OpenAI client (chá»‰ dÃ¹ng Ä‘á»ƒ tráº£ lá»i, khÃ´ng embed)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

router = APIRouter(prefix="/api/chatbot", tags=["Chatbot"])

# ----- ChromaDB local -----
chroma_client = chromadb.PersistentClient(path="app/AI/vector_db")
collection = chroma_client.get_collection("medical_rag")

# ----- Load local BGE model Ä‘á»ƒ embed cÃ¢u há»i -----
print("Loading BGE-small for query embedding...")
embed_model = SentenceTransformer("BAAI/bge-small-en")

SYSTEM_PROMPT = """
Báº¡n lÃ  trá»£ lÃ½ y táº¿ an toÃ n, chá»‰ há»— trá»£ cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n sá»©c khá»e vÃ  y táº¿.

QUY Táº®C:
- Chá»‰ tráº£ lá»i cÃ¡c cÃ¢u há»i thuá»™c lÄ©nh vá»±c y táº¿, triá»‡u chá»©ng, bá»‡nh, chÄƒm sÃ³c sá»©c khá»e, sÆ¡ cá»©u, hÆ°á»›ng dáº«n an toÃ n.
- Náº¿u cÃ¢u há»i KHÃ”NG liÃªn quan Ä‘áº¿n y táº¿ -> pháº£i tráº£ lá»i: 
  "TÃ´i chá»‰ há»— trá»£ cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n sá»©c khá»e vÃ  y táº¿."
- Æ¯u tiÃªn dÃ¹ng dá»¯ liá»‡u tá»« RAG.
- Náº¿u RAG khÃ´ng Ä‘á»§, cÃ³ thá»ƒ Ä‘Æ°a ra lá»i khuyÃªn chung chung nhÆ°ng pháº£i liÃªn quan Ä‘áº¿n sá»©c khá»e.
- KHÃ”NG cháº©n Ä‘oÃ¡n bá»‡nh chÃ­nh xÃ¡c.
- KHÃ”NG kÃª thuá»‘c.
- KHÃ”NG tá»± káº¿t luáº­n bá»‡nh.
- CÃ³ thá»ƒ Ä‘Æ°a ra hÆ°á»›ng dáº«n chÄƒm sÃ³c cÆ¡ báº£n (uá»‘ng nÆ°á»›c, nghá»‰ ngÆ¡i, theo dÃµi triá»‡u chá»©ng).
- Náº¿u triá»‡u chá»©ng nguy hiá»ƒm (khÃ³ thá»Ÿ, Ä‘au ngá»±c, lÆ¡ mÆ¡, sá»‘t cao kÃ©o dÃ i...) -> yÃªu cáº§u ngÆ°á»i dÃ¹ng Ä‘áº¿n bá»‡nh viá»‡n ngay.
- LuÃ´n Ä‘Æ°a ra 1â€“3 gá»£i Ã½ hÃ nh Ä‘á»™ng an toÃ n.

YÃŠU Cáº¦U TRáº¢ Lá»œI:
- Ngáº¯n gá»n, dá»… hiá»ƒu, tiáº¿ng Viá»‡t.
- KhÃ´ng nÃ³i vá» cÃ¡c chá»§ Ä‘á» ngoÃ i y táº¿.
"""

class UserMessage(BaseModel):
    message: str

# ----- RAG Retrieval -----
def retrieve_context(query: str):
    # Táº¡o embedding cÃ¢u há»i báº±ng BGE-small
    query_vec = embed_model.encode([query]).tolist()

    # Query báº±ng vector, KHÃ”NG dÃ¹ng query_texts
    result = collection.query(
        query_embeddings=query_vec,
        n_results=3
    )
    
    docs = result["documents"][0]
    return "\n\n".join(docs)

@router.post("")
async def chatbot(msg: UserMessage):
    user_input = msg.message
    context = retrieve_context(user_input)

    # ğŸ”’ BLOCK: Náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u y táº¿ trong RAG â†’ cháº·n cÃ¢u há»i khÃ´ng liÃªn quan
    if context.strip() == "":
        return {
            "reply": "TÃ´i chá»‰ há»— trá»£ cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n sá»©c khá»e vÃ  y táº¿."
        }

    prompt = f"""
{SYSTEM_PROMPT}

Dá»¯ liá»‡u RAG thu Ä‘Æ°á»£c:
{context}

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:
{user_input}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )

    return {"reply": response.choices[0].message.content}

